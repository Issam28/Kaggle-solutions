{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 433)\n",
      "(506691, 432)\n",
      "CPU times: user 41.8 s, sys: 4.33 s, total: 46.1 s\n",
      "Wall time: 60 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_transaction = pd.read_csv('input/train_transaction.csv', index_col='TransactionID')\n",
    "test_transaction = pd.read_csv('input/test_transaction.csv', index_col='TransactionID')\n",
    "\n",
    "\n",
    "train_identity = pd.read_csv('input/train_identity.csv', index_col='TransactionID')\n",
    "test_identity = pd.read_csv('input/test_identity.csv', index_col='TransactionID')\n",
    "\n",
    "sample_submission = pd.read_csv('input/sample_submission.csv', index_col='TransactionID')\n",
    "\n",
    "train_df = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\n",
    "test_df = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "\n",
    "train_label = train_df['isFraud'].copy()\n",
    "train_df = train_df.drop('isFraud', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['online']=0\n",
    "test_df['online']=0\n",
    "train_df.loc[train_identity.index.values,'online']=1\n",
    "test_df.loc[test_identity.index.values,'online']=1\n",
    "\n",
    "train_df['missing']=train_df.isnull().sum(axis=1)\n",
    "test_df['missing']=test_df.isnull().sum(axis=1)\n",
    "\n",
    "train_df=train_df.fillna(-999)\n",
    "test_df=test_df.fillna(-999)\n",
    "\n",
    "train_df=train_df.reset_index(drop=True)\n",
    "train_label=train_label.reset_index(drop=True)\n",
    "test_df=test_df.reset_index(drop=True)\n",
    "\n",
    "del train_transaction, train_identity, test_transaction, test_identity\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards=[i for i in train_df.columns if i.startswith('card')]\n",
    "addr=[i for i in train_df.columns if i.startswith('addr')]\n",
    "dist=[i for i in train_df.columns if i.startswith('dist')]\n",
    "emaildomain=[i for i in train_df.columns if i.endswith('emaildomain')]\n",
    "_bin=[i+'_bin' for i in emaildomain]\n",
    "_suffix=[i+'_suffix' for i in emaildomain]\n",
    "C=[i for i in train_df.columns if i.startswith('C')]\n",
    "D=[i for i in train_df.columns if i.startswith('D') and i[1]!='e']\n",
    "M=[i for i in train_df.columns if i.startswith('M')]\n",
    "V=[i for i in train_df.columns if i.startswith('V')]\n",
    "\n",
    "id_=[i for i in train_df.columns if i.startswith('id')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'protonmail', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n",
    "us_emails = ['gmail', 'net', 'edu']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78487e3aeabb4db08de2447a0fd5539d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=64), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from itertools import combinations\n",
    "def agg_feat(train_df,test_df,grp_cols):\n",
    "    count=0\n",
    "    new_name=\"\"\n",
    "    for c in grp_cols:\n",
    "        new_name += c+'_'\n",
    "    for c in grp_cols:\n",
    "        if count==0:\n",
    "            train_df[new_name] = train_df[c].astype(str)+'_'\n",
    "            test_df[new_name] = test_df[c].astype(str)+'_'\n",
    "            count+=1\n",
    "        else:\n",
    "            train_df[new_name] += train_df[c].astype(str)+'_'\n",
    "            test_df[new_name] += test_df[c].astype(str)+'_'\n",
    "            \n",
    "    return train_df,test_df\n",
    "\n",
    "#for f in tqdm(output):\n",
    "#    if len(f)>1:\n",
    "#        in_=[item for sublist in f for item in sublist]\n",
    "#        #print(in_)\n",
    "#        train_df,test_df=agg_feat(train_df,test_df,in_)\n",
    "        \n",
    "\n",
    "\n",
    "train_df,test_df=agg_feat(train_df,test_df,['C1','C14'])\n",
    "train_df,test_df=agg_feat(train_df,test_df,['C13','C14'])\n",
    "train_df,test_df=agg_feat(train_df,test_df,['C8','C14'])\n",
    "train_df,test_df=agg_feat(train_df,test_df,['C1','C13'])\n",
    "train_df,test_df=agg_feat(train_df,test_df,['V258','V201'])\n",
    "train_df,test_df=agg_feat(train_df,test_df,['V201','V244'])\n",
    "train_df,test_df=agg_feat(train_df,test_df,['V258','V244'])\n",
    "train_df,test_df=agg_feat(train_df,test_df,['V258','V262'])\n",
    "train_df,test_df=agg_feat(train_df,test_df,['V201','V244'])\n",
    "train_df,test_df=agg_feat(train_df,test_df,['addr1','card1'])\n",
    "train_df,test_df=agg_feat(train_df,test_df,['addr1','card2'])\n",
    "train_df,test_df=agg_feat(train_df,test_df,['addr2','card1'])\n",
    "train_df,test_df=agg_feat(train_df,test_df,['addr2','card1'])\n",
    "train_df,test_df=agg_feat(train_df,test_df,['C13','V258'])\n",
    "train_df,test_df=agg_feat(train_df,test_df,['C1','V258'])\n",
    "train_df,test_df=agg_feat(train_df,test_df,['C1','V201'])\n",
    "train_df,test_df=agg_feat(train_df,test_df,['C14','V294'])\n",
    "\n",
    "input_ = [[i] for i in train_df.columns if i.startswith('card')]\n",
    "output = sum([list(map(list, combinations(input_, i))) for i in range(len(input_) + 1)], [])\n",
    "for f in tqdm(output):\n",
    "    if len(f)>1:\n",
    "        in_=[item for sublist in f for item in sublist]\n",
    "        #print(in_)\n",
    "        train_df,test_df=agg_feat(train_df,test_df,in_)\n",
    "\n",
    "train_df,test_df=agg_feat(train_df,test_df,['TransactionAmt','ProductCD']+cards)\n",
    "train_df,test_df=agg_feat(train_df,test_df,['ProductCD']+cards)        \n",
    "train_df,test_df=agg_feat(train_df,test_df,['TransactionAmt','ProductCD'])\n",
    "#train_df,test_df=agg_feat(train_df,test_df,cards)\n",
    "train_df,test_df=agg_feat(train_df,test_df,addr)\n",
    "train_df,test_df=agg_feat(train_df,test_df,cards+addr)\n",
    "train_df,test_df=agg_feat(train_df,test_df,cards+addr+emaildomain)\n",
    "train_df,test_df=agg_feat(train_df,test_df,cards+addr+emaildomain+['ProductCD']+cards)\n",
    "train_df,test_df=agg_feat(train_df,test_df,emaildomain)\n",
    "\n",
    "train_df,test_df=agg_feat(train_df,test_df,C)\n",
    "train_df,test_df=agg_feat(train_df,test_df,M)\n",
    "train_df,test_df=agg_feat(train_df,test_df,id_)\n",
    "\n",
    "#https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499#latest-579654\n",
    "for c in ['P_emaildomain', 'R_emaildomain']:\n",
    "    train_df[c + '_bin'] = train_df[c].map(emails)\n",
    "    test_df[c + '_bin'] = test_df[c].map(emails)\n",
    "    \n",
    "    train_df[c + '_suffix'] = train_df[c].map(lambda x: str(x).split('.')[-1])\n",
    "    test_df[c + '_suffix'] = test_df[c].map(lambda x: str(x).split('.')[-1])\n",
    "    \n",
    "    train_df[c + '_suffix'] = train_df[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "    test_df[c + '_suffix'] = test_df[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "\n",
    "train_df['email_domain_comp'] = (train_df['P_emaildomain'].values == train_df['R_emaildomain'].values).astype(int)\n",
    "test_df['email_domain_comp'] = (test_df['P_emaildomain'].values == test_df['R_emaildomain'].values).astype(int)\n",
    "\n",
    "train_df['email_domain_suffix_bin'] = (train_df['P_emaildomain_bin'].values == train_df['R_emaildomain_bin'].values).astype(int)\n",
    "test_df['email_domain_suffix_bin'] = (test_df['P_emaildomain_bin'].values == test_df['R_emaildomain_bin'].values).astype(int)\n",
    "\n",
    "train_df['email_domain_suffix_comp'] = (train_df['P_emaildomain_suffix'].values == train_df['R_emaildomain_suffix'].values).astype(int)\n",
    "test_df['email_domain_suffix_comp'] = (test_df['P_emaildomain_suffix'].values == test_df['R_emaildomain_suffix'].values).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5df1191f984bc79a880c84b9297d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=524), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Label Encoding\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "for f in tqdm(train_df.columns):\n",
    "    if train_df[f].dtype=='object' or test_df[f].dtype=='object': \n",
    "        tmp = pd.factorize(pd.concat([train_df[f],test_df[f]],axis=0), sort=False)[0].astype('int32')\n",
    "        train_df[f]=tmp[:train_df.shape[0],]\n",
    "        test_df[f]=tmp[train_df.shape[0]:,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b9d93e8643c4d02a538068b4157839d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=176), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# add 1-way grouping count-based features\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "new=[i for i in train_df.columns if i.endswith('_')]\n",
    "cols=list(cards+addr+emaildomain+dist+C+D+M+id_+_bin+['DeviceType','DeviceInfo','ProductCD']+new)\n",
    "for col in tqdm(cols):\n",
    "    d = pd.concat([train_df[col],test_df[col]],axis=0).value_counts(dropna=False).to_dict()\n",
    "    train_df[col+\"_c\"] = train_df[col].apply(lambda x:d.get(x,-999))\n",
    "    test_df[col+\"_c\"] = test_df[col].apply(lambda x:d.get(x,-999))\n",
    "    del d\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24cc228a10584e4ab7217145887378e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=17), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#add 2-way grouping count-based features\n",
    "import itertools\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "#['new_TransactionAmt','C13'],['new_TransactionAmt','V258']\n",
    "cols=[['C1','C14'],['C13','C14'],['addr1','card1'],['card1','card2'],['C14','C8'],['C13','V258'],['C1','V258'],['V189','V258'],['C1','V201'],[\"C1\",\"C13\"],['V258','V262'],['C14','V294'],['V258','V244'],['V258','V201'],['V201','V244'],['V225','V258'],['V258','V294']]\n",
    "#cols=C+addr+cards+['V258','V189','V244','V201','V294']\n",
    "#cols=list(itertools.combinations(cols, 2))\n",
    "for col in tqdm(cols):\n",
    "    #col=list(col)\n",
    "    tmp=pd.concat([train_df[col],test_df[col]],axis=0).groupby(by=col)[col[0]].transform('count')\n",
    "    train_df[col[0]+col[1]+'_c']=tmp.iloc[0:train_df.shape[0],]\n",
    "    test_df[col[0]+col[1]+'_c']=tmp.iloc[train_df.shape[0]:,]\n",
    "    del tmp\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d9f8c4acda40d2bdd185cc3d9c6f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#add 3-way grouping count-based features\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "cols=[['C1','C14','V201'],['C1','V189','V258'],['C14','C8','V294'],['C1','V201','V244'],['C1','V130','V258'],['C14','C8','D2'],['V189','V258','V294'],['C1','V201','V209'],['C14','V201','V294']]\n",
    "for col in tqdm(cols):\n",
    "    tmp=pd.concat([train_df[col],test_df[col]],axis=0).groupby(by=col)[col[0]].transform('count')\n",
    "    train_df[col[0]+col[1]+col[2]+'_c']=tmp.iloc[0:train_df.shape[0],]\n",
    "    test_df[col[0]+col[1]+col[2]+'_c']=tmp.iloc[train_df.shape[0]:,]\n",
    "    del tmp\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col=['TransactionAmt','card1','card4']\n",
    "cnt=pd.concat([train_df[col],test_df[col]],axis=0)\n",
    "cnt['TransactionAmt_to_mean_card1'] = cnt['TransactionAmt'] / cnt.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
    "cnt['TransactionAmt_to_mean_card4'] = cnt['TransactionAmt'] / cnt.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "cnt['TransactionAmt_to_std_card1'] = cnt['TransactionAmt'] / cnt.groupby(['card1'])['TransactionAmt'].transform('std')\n",
    "cnt['TransactionAmt_to_std_card4'] = cnt['TransactionAmt'] / cnt.groupby(['card4'])['TransactionAmt'].transform('std')\n",
    "\n",
    "train_df['TransactionAmt_to_mean_card1']=cnt['TransactionAmt_to_mean_card1'].iloc[0:train_df.shape[0],]\n",
    "test_df['TransactionAmt_to_mean_card1']=cnt['TransactionAmt_to_mean_card1'].iloc[train_df.shape[0]:,]\n",
    "\n",
    "train_df['TransactionAmt_to_mean_card4']=cnt['TransactionAmt_to_mean_card4'].iloc[0:train_df.shape[0],]\n",
    "test_df['TransactionAmt_to_mean_card4']=cnt['TransactionAmt_to_mean_card4'].iloc[train_df.shape[0]:,]\n",
    "\n",
    "train_df['TransactionAmt_to_std_card1']=cnt['TransactionAmt_to_std_card1'].iloc[0:train_df.shape[0],]\n",
    "test_df['TransactionAmt_to_std_card1']=cnt['TransactionAmt_to_std_card1'].iloc[train_df.shape[0]:,]\n",
    "\n",
    "train_df['TransactionAmt_to_std_card4']=cnt['TransactionAmt_to_std_card4'].iloc[0:train_df.shape[0],]\n",
    "test_df['TransactionAmt_to_std_card4']=cnt['TransactionAmt_to_std_card4'].iloc[train_df.shape[0]:,]\n",
    "\n",
    "del cnt\n",
    "gc.collect()\n",
    "\n",
    "col=['id_02','card1','card4']\n",
    "cnt=pd.concat([train_df[col],test_df[col]],axis=0)\n",
    "cnt['id_02_to_mean_card1'] = cnt['id_02'] / cnt.groupby(['card1'])['id_02'].transform('mean')\n",
    "cnt['id_02_to_mean_card4'] = cnt['id_02'] / cnt.groupby(['card4'])['id_02'].transform('mean')\n",
    "cnt['id_02_to_std_card1'] = cnt['id_02'] / cnt.groupby(['card1'])['id_02'].transform('std')\n",
    "cnt['id_02_to_std_card4'] = cnt['id_02'] / cnt.groupby(['card4'])['id_02'].transform('std')\n",
    "\n",
    "train_df['id_02_to_mean_card1']=cnt['id_02_to_mean_card1'].iloc[0:train_df.shape[0],]\n",
    "test_df['id_02_to_mean_card1']=cnt['id_02_to_mean_card1'].iloc[train_df.shape[0]:,]\n",
    "\n",
    "train_df['id_02_to_mean_card4']=cnt['id_02_to_mean_card4'].iloc[0:train_df.shape[0],]\n",
    "test_df['id_02_to_mean_card4']=cnt['id_02_to_mean_card4'].iloc[train_df.shape[0]:,]\n",
    "\n",
    "train_df['id_02_to_std_card1']=cnt['id_02_to_std_card1'].iloc[0:train_df.shape[0],]\n",
    "test_df['id_02_to_std_card1']=cnt['id_02_to_std_card1'].iloc[train_df.shape[0]:,]\n",
    "\n",
    "train_df['id_02_to_std_card4']=cnt['id_02_to_std_card4'].iloc[0:train_df.shape[0],]\n",
    "test_df['id_02_to_std_card4']=cnt['id_02_to_std_card4'].iloc[train_df.shape[0]:,]\n",
    "\n",
    "del cnt\n",
    "gc.collect()\n",
    "\n",
    "col=['D15','card1','card4']\n",
    "cnt=pd.concat([train_df[col],test_df[col]],axis=0)\n",
    "cnt['D15_to_mean_card1'] = cnt['D15'] / cnt.groupby(['card1'])['D15'].transform('mean')\n",
    "cnt['D15_to_mean_card4'] = cnt['D15'] / cnt.groupby(['card4'])['D15'].transform('mean')\n",
    "cnt['D15_to_std_card1'] = cnt['D15'] / cnt.groupby(['card1'])['D15'].transform('std')\n",
    "cnt['D15_to_std_card4'] = cnt['D15'] / cnt.groupby(['card4'])['D15'].transform('std')\n",
    "\n",
    "train_df['D15_to_mean_card1']=cnt['D15_to_mean_card1'].iloc[0:train_df.shape[0],]\n",
    "test_df['D15_to_mean_card1']=cnt['D15_to_mean_card1'].iloc[train_df.shape[0]:,]\n",
    "\n",
    "train_df['D15_to_mean_card4']=cnt['D15_to_mean_card4'].iloc[0:train_df.shape[0],]\n",
    "test_df['D15_to_mean_card4']=cnt['D15_to_mean_card4'].iloc[train_df.shape[0]:,]\n",
    "\n",
    "train_df['D15_to_std_card1']=cnt['D15_to_std_card1'].iloc[0:train_df.shape[0],]\n",
    "test_df['D15_to_std_card1']=cnt['D15_to_std_card1'].iloc[train_df.shape[0]:,]\n",
    "\n",
    "train_df['D15_to_std_card4']=cnt['D15_to_std_card4'].iloc[0:train_df.shape[0],]\n",
    "test_df['D15_to_std_card4']=cnt['D15_to_std_card4'].iloc[train_df.shape[0]:,]\n",
    "\n",
    "del cnt\n",
    "gc.collect()\n",
    "\n",
    "col=['D15','addr1','card4']\n",
    "cnt=pd.concat([train_df[col],test_df[col]],axis=0)\n",
    "cnt['D15_to_mean_addr1'] = cnt['D15'] / cnt.groupby(['addr1'])['D15'].transform('mean')\n",
    "cnt['D15_to_mean_card4'] = cnt['D15'] / cnt.groupby(['card4'])['D15'].transform('mean')\n",
    "cnt['D15_to_std_addr1'] = cnt['D15'] / cnt.groupby(['addr1'])['D15'].transform('std')\n",
    "cnt['D15_to_std_card4'] = cnt['D15'] / cnt.groupby(['card4'])['D15'].transform('std')\n",
    "\n",
    "train_df['D15_to_mean_addr1']=cnt['D15_to_mean_addr1'].iloc[0:train_df.shape[0],]\n",
    "test_df['D15_to_mean_addr1']=cnt['D15_to_mean_addr1'].iloc[train_df.shape[0]:,]\n",
    "\n",
    "train_df['D15_to_mean_card4']=cnt['D15_to_mean_card4'].iloc[0:train_df.shape[0],]\n",
    "test_df['D15_to_mean_card4']=cnt['D15_to_mean_card4'].iloc[train_df.shape[0]:,]\n",
    "\n",
    "train_df['D15_to_std_addr1']=cnt['D15_to_std_addr1'].iloc[0:train_df.shape[0],]\n",
    "test_df['D15_to_std_addr1']=cnt['D15_to_std_addr1'].iloc[train_df.shape[0]:,]\n",
    "\n",
    "train_df['D15_to_std_card4']=cnt['D15_to_std_card4'].iloc[0:train_df.shape[0],]\n",
    "test_df['D15_to_std_card4']=cnt['D15_to_std_card4'].iloc[train_df.shape[0]:,]\n",
    "\n",
    "del cnt\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/issam/anaconda3/envs/testing/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in log1p\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/issam/anaconda3/envs/testing/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in log1p\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Log1p transformation for TransactionAmt\n",
    "log_cols=['TransactionAmt']+dist\n",
    "train_df[log_cols]=np.log1p(train_df[log_cols])\n",
    "test_df[log_cols]=np.log1p(test_df[log_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grp_agg(train_df,test_df,agg,grpby):\n",
    "    d = pd.concat([train_df[agg+grpby],test_df[agg+grpby]],axis=0).groupby(grpby).mean().to_dict()\n",
    "    train_df['mean'+str(grpby[0])] = train_df[grpby[0]].apply(lambda x:d[agg[0]].get(x,-999))\n",
    "    test_df[\"mean\"+str(grpby[0])] = test_df[grpby[0]].apply(lambda x:d[agg[0]].get(x,-999))\n",
    "    \n",
    "    d = pd.concat([train_df[agg+grpby],test_df[agg+grpby]],axis=0).groupby(grpby).sum().to_dict()\n",
    "    train_df['sum'+str(grpby[0])] = train_df[grpby[0]].apply(lambda x:d[agg[0]].get(x,-999))\n",
    "    test_df[\"sum\"+str(grpby[0])] = test_df[grpby[0]].apply(lambda x:d[agg[0]].get(x,-999))\n",
    "\n",
    "    d = pd.concat([train_df[agg+grpby],test_df[agg+grpby]],axis=0).groupby(grpby).min().to_dict()\n",
    "    train_df['min'+str(grpby[0])] = train_df[grpby[0]].apply(lambda x:d[agg[0]].get(x,-999))\n",
    "    test_df[\"min\"+str(grpby[0])] = test_df[grpby[0]].apply(lambda x:d[agg[0]].get(x,-999))\n",
    "\n",
    "    d = pd.concat([train_df[agg+grpby],test_df[agg+grpby]],axis=0).groupby(grpby).max().to_dict()\n",
    "    train_df['max'+str(grpby[0])] = train_df[grpby[0]].apply(lambda x:d[agg[0]].get(x,-999))\n",
    "    test_df[\"max\"+str(grpby[0])] = test_df[grpby[0]].apply(lambda x:d[agg[0]].get(x,-999))\n",
    "\n",
    "    d = pd.concat([train_df[agg+grpby],test_df[agg+grpby]],axis=0).groupby(grpby).std().to_dict()\n",
    "    train_df['std'+str(grpby[0])] = train_df[grpby[0]].apply(lambda x:d[agg[0]].get(x,-999))\n",
    "    test_df[\"std\"+str(grpby[0])] = test_df[grpby[0]].apply(lambda x:d[agg[0]].get(x,-999))\n",
    "\n",
    "    d = pd.concat([train_df[agg+grpby],test_df[agg+grpby]],axis=0).groupby(grpby).median().to_dict()\n",
    "    train_df['median'+str(grpby[0])] = train_df[grpby[0]].apply(lambda x:d[agg[0]].get(x,-999))\n",
    "    test_df[\"median\"+str(grpby[0])] = test_df[grpby[0]].apply(lambda x:d[agg[0]].get(x,-999))\n",
    "    \n",
    "    return train_df,test_df\n",
    "\n",
    "\n",
    "\n",
    "train_df,test_df=grp_agg(train_df,test_df,['TransactionAmt'],['card1'])\n",
    "train_df,test_df=grp_agg(train_df,test_df,['TransactionAmt'],['card2'])\n",
    "#train_df,test_df=grp_agg(train_df,test_df,['TransactionAmt'],['card3'])\n",
    "train_df,test_df=grp_agg(train_df,test_df,['TransactionAmt'],['card1_card2_card3_card4_card5_card6_'])\n",
    "\n",
    "train_df,test_df=grp_agg(train_df,test_df,['TransactionAmt'],['addr1'])\n",
    "train_df,test_df=grp_agg(train_df,test_df,['TransactionAmt'],['addr2'])\n",
    "train_df,test_df=grp_agg(train_df,test_df,['TransactionAmt'],['addr1_addr2_'])\n",
    "\n",
    "train_df,test_df=grp_agg(train_df,test_df,['TransactionAmt'],['P_emaildomain'])\n",
    "train_df,test_df=grp_agg(train_df,test_df,['TransactionAmt'],['R_emaildomain'])\n",
    "\n",
    "train_df,test_df=grp_agg(train_df,test_df,['C14'],['C1'])\n",
    "train_df,test_df=grp_agg(train_df,test_df,['C14'],['C13'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "START_DATE = '2017-12-01'\n",
    "startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "train_df['TransactionDT'] = train_df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n",
    "\n",
    "#train_df['year'] = train_df['TransactionDT'].dt.year\n",
    "#train_df['month'] = train_df['TransactionDT_new'].dt.month\n",
    "#train_df['week'] = train_df['TransactionDT'].dt.week\n",
    "train_df['dow'] = train_df['TransactionDT'].dt.dayofweek\n",
    "train_df['hour'] = train_df['TransactionDT'].dt.hour\n",
    "train_df['day'] = train_df['TransactionDT'].dt.day\n",
    "train_df['wom'] = train_df['TransactionDT'].apply(lambda d: (d.day-1) // 7 + 1)\n",
    "\n",
    "test_df['TransactionDT'] = test_df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n",
    "\n",
    "#test_df['year'] = test_df['TransactionDT'].dt.year\n",
    "#test_df['month'] = test_df['TransactionDT_new'].dt.month\n",
    "#test_df['week'] = test_df['TransactionDT'].dt.week\n",
    "test_df['dow'] = test_df['TransactionDT'].dt.dayofweek\n",
    "test_df['hour'] = test_df['TransactionDT'].dt.hour\n",
    "test_df['day'] = test_df['TransactionDT'].dt.day\n",
    "test_df['wom'] = test_df['TransactionDT'].apply(lambda d: (d.day-1) // 7 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(['TransactionDT'],axis=1,inplace=True)\n",
    "test_df.drop(['TransactionDT'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#Delete some columns\n",
    "cols = ['V300', 'V309', 'V111', 'V124', 'V106', 'V125', 'V315', 'V134', 'V102', 'V123', 'V316', 'V113', 'V136', 'V305',\n",
    "        'V110', 'V299', 'V289', 'V286', 'V318', 'V103', 'V304', 'V116', 'V298', 'V284', 'V293', 'V137', 'V295', 'V301',\n",
    "        'V104', 'V311', 'V115', 'V109', 'V119', 'V321', 'V114', 'V133', 'V122', 'V319', 'V105', 'V112', 'V118', 'V117',\n",
    "        'V121', 'V108', 'V135', 'V320', 'V303', 'V297', 'V120']\n",
    "\n",
    "one_value_cols = [col for col in train_df.columns if train_df[col].nunique() <= 1]\n",
    "one_value_cols_test_df = [col for col in test_df.columns if test_df[col].nunique() <= 1]\n",
    "print(one_value_cols == one_value_cols_test_df)\n",
    "\n",
    "many_null_cols = [col for col in train_df.columns if train_df[col].isnull().sum() / train_df.shape[0] > 0.9]\n",
    "many_null_cols_test_df = [col for col in test_df.columns if test_df[col].isnull().sum() / test_df.shape[0] > 0.9]\n",
    "big_top_value_cols = [col for col in train_df.columns if train_df[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n",
    "big_top_value_cols_test_df = [col for col in test_df.columns if test_df[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n",
    "\n",
    "cols_to_drop = list(set(cols+many_null_cols + many_null_cols_test_df +\n",
    "                        big_top_value_cols +\n",
    "                        big_top_value_cols_test_df +\n",
    "                        one_value_cols+ one_value_cols_test_df))\n",
    "print(len(cols_to_drop))\n",
    "\n",
    "train_df = train_df.drop(cols_to_drop, axis=1)\n",
    "test_df = test_df.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((590540, 703), (506691, 703))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape,test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encode categorical features\n",
    "from scipy import sparse as ssp\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_features=addr+emaildomain+_bin+_suffix+cards+M+['ProductCD','DeviceType','DeviceInfo']#+id_\n",
    "print(len(cat_features))\n",
    "rest_feats=[i for i in train_df.columns if i not in cat_features]\n",
    "\n",
    "enc = OneHotEncoder(categories='auto')\n",
    "enc.fit(list(train_df[cat_features].values)+list(test_df[cat_features].values))\n",
    "X_cat = enc.transform(train_df[cat_features])\n",
    "X_t_cat = enc.transform(test_df[cat_features])\n",
    "\n",
    "train_list = [train_df[rest_feats].values,X_cat,]\n",
    "test_list = [test_df[rest_feats].values,X_t_cat,]\n",
    "\n",
    "train_df = ssp.hstack(train_list).tocsr()\n",
    "test_df = ssp.hstack(test_list).tocsr()\n",
    "\n",
    "del X_cat,X_t_cat,train_list,test_list,enc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472432\n",
      "\n",
      "\n",
      "Fold #: 5\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's auc: 0.942502\tvalid's auc: 0.897483\n",
      "[200]\ttrain's auc: 0.967373\tvalid's auc: 0.91138\n",
      "[300]\ttrain's auc: 0.980742\tvalid's auc: 0.920846\n",
      "[400]\ttrain's auc: 0.98854\tvalid's auc: 0.927133\n",
      "[500]\ttrain's auc: 0.993174\tvalid's auc: 0.930932\n",
      "[600]\ttrain's auc: 0.996003\tvalid's auc: 0.933215\n",
      "[700]\ttrain's auc: 0.997692\tvalid's auc: 0.934729\n",
      "[800]\ttrain's auc: 0.998625\tvalid's auc: 0.935738\n",
      "[900]\ttrain's auc: 0.999162\tvalid's auc: 0.936576\n",
      "[1000]\ttrain's auc: 0.999496\tvalid's auc: 0.937115\n",
      "[1100]\ttrain's auc: 0.999692\tvalid's auc: 0.937489\n",
      "[1200]\ttrain's auc: 0.999804\tvalid's auc: 0.93769\n",
      "[1300]\ttrain's auc: 0.999879\tvalid's auc: 0.937608\n",
      "Early stopping, best iteration is:\n",
      "[1223]\ttrain's auc: 0.999825\tvalid's auc: 0.937765\n",
      "Score for each fold 0.9377646810479559\n",
      "\n",
      "\n",
      "str:  0.0 fold scores:  [0.9377646810479559]\n",
      "cv score:  0.5155101204484595\n",
      "current score:  0.5155101204484595 1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LightGBM classifier with bagging\n",
    "\"\"\"\n",
    "\n",
    "import lightgbm as lgbm\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "\n",
    "bootstrap=True\n",
    "subsample=1\n",
    "shuffle=True\n",
    "test_id=test_df.shape[0]\n",
    "num_boost_round=20000\n",
    "\n",
    "final_cv_train = np.zeros(len(train_label))\n",
    "final_cv_pred = np.zeros(test_id)\n",
    "\n",
    "NFOLDS = 5\n",
    "\n",
    "random_seeds=[5736]#,45,123,9632]\n",
    "num_bagg=len(random_seeds)\n",
    "for s in range(0,num_bagg):\n",
    "    kfold = KFold(NFOLDS, shuffle=True, random_state=random_seeds[s])\n",
    "    #ix = np.random.choice(len(train_label), int(subsample * len(train_label)), bootstrap)\n",
    "    #if not shuffle:\n",
    "    #    ix=np.sort(ix)\n",
    "    cv_train = np.zeros(len(train_label))\n",
    "    cv_pred = np.zeros(test_id)\n",
    "    \n",
    "    params = {\n",
    "    'n_jobs' : -1 ,\n",
    "    'device': 'cpu', \n",
    "    \"objective\": \"binary\",\n",
    "    'metric': \"auc\",#'binary_logloss',\n",
    "    \"boosting_type\": \"gbdt\", \n",
    "    'learning_rate': 0.01,\n",
    "    #'max_depth': 10,\n",
    "    'num_leaves': 2**8-1,\n",
    "    'bagging_fraction': 0.9,\n",
    "    'feature_fraction': 0.2,\n",
    "    'bagging_freq': 1,\n",
    "    #'boost_from_average':'false',\n",
    "    #'min_child_samples': 50,\n",
    "    #'min_child_weight': 100.0,\n",
    "    #'min_split_gain': 0.1, \n",
    "    #'lambda_l1':0.05,\n",
    "    #'lambda_l2':0.05,\n",
    "    #\"max_bin\": 64,\n",
    "    \"seed\":random_seeds[s]+1,\n",
    "    'verbosity': 1\n",
    "    }\n",
    "    \n",
    "\n",
    "    kf = kfold.split(train_df, train_label)\n",
    "    #train_df=pd.concant([train_df,scaleddf_train],axis=1)\n",
    "    best_trees = []\n",
    "    fold_scores = []\n",
    "    split=int(0.8*len(train_label))\n",
    "    print(split)\n",
    "    for i, (train_fold, validate) in enumerate(kf):\n",
    "        if i!=4:\n",
    "            continue\n",
    "        print(\"\\n\\nFold #:\",i+1)\n",
    "        #train_fold=np.random.permutation(np.concatenate((train_fold,index_zero)))\n",
    "        \n",
    "        X_train, X_validate, label_train, label_validate = train_df[:split, :], train_df[split:, :], train_label[:split], train_label[split:]\n",
    "        dtrain = lgbm.Dataset(X_train, label_train)\n",
    "        \n",
    "        del X_train,label_train\n",
    "        gc.collect()\n",
    "        \n",
    "        #Train the model\n",
    "        dvalid = lgbm.Dataset(X_validate, label_validate, reference=dtrain)\n",
    "        bst = lgbm.train(params, dtrain,valid_sets=[dtrain,dvalid],valid_names=['train','valid'],num_boost_round=num_boost_round, verbose_eval=100, early_stopping_rounds=100)\n",
    "        best_trees.append(bst.best_iteration)\n",
    "        \n",
    "        #Predict on validation set\n",
    "        cv_train[split:] += bst.predict(X_validate,num_iteration=bst.best_iteration)\n",
    "        score = roc_auc_score(label_validate, cv_train[split:])\n",
    "        print (\"Score for each fold\",score)\n",
    "        fold_scores.append(score)\n",
    "    \n",
    "        # Free some memory\n",
    "        del dtrain,dvalid, X_validate, label_validate\n",
    "        gc.collect()\n",
    "        \n",
    "        # Predict on Test set\n",
    "        #cv_pred += bst.predict(test_df, num_iteration=bst.best_iteration)\n",
    "\n",
    "    cv_pred /= NFOLDS\n",
    "    final_cv_train += cv_train\n",
    "    final_cv_pred += cv_pred\n",
    "\n",
    "    print(\"\\n\\nstr: \",np.array(fold_scores).std(), \"fold scores: \",fold_scores)\n",
    "    print(\"cv score: \", roc_auc_score(train_label, cv_train))\n",
    "    print(\"current score: \", roc_auc_score(train_label, final_cv_train / (s+1)), s+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
