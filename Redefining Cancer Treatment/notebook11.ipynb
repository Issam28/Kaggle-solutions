{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a3076f31-7052-4e2c-ac9e-f6fd93d18a86",
    "_uuid": "0d77c2144b6b5523aeb22d0ea999b216ebbe56fa"
   },
   "source": [
    "# Donated to Cancer Treatment Too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "217a645a-29bc-4f2d-8795-24fb4f55d94d",
    "_uuid": "8abb336a1fb5b8c4e23b41efdaa9207c95b5974e"
   },
   "source": [
    "* Note: As this is my first public Kernel and I'm still in learning NLP, please feel free to comment if you have any questions.\n",
    "* This Kernel is modified from [the1owl: Redefining Treatment](https://www.kaggle.com/the1owl/redefining-treatment-0-57456), really thanks!\n",
    "* I will highlight the main differences from the original.\n",
    "* Finnally, donated to cancer treatment too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "56f93ab8-fa0f-4bf4-9b30-625c2eda8d15",
    "_uuid": "8c73ab9999e9d01e83d3a986e7bd1a13f2b800c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seven\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing, pipeline, feature_extraction, decomposition, model_selection, metrics, cross_validation, svm\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import normalize, Imputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "c2340896-53c5-467b-8087-1c1136cf87a8",
    "_uuid": "6c717a3ad79b2144092286757adeff5dd6f6945e",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('input/training_variants')\n",
    "test = pd.read_csv('input/test_variants.csv')\n",
    "trainx = pd.read_csv('input/training_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
    "testx = pd.read_csv('input/test_text.csv', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
    "\n",
    "sol=pd.read_csv('input/stage1_solution_filtered.csv')\n",
    "testx1 = pd.read_csv('input/test_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
    "test1 = pd.read_csv('input/test_variants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.614285714286\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "    assert( len(actual) == len(pred) )\n",
    "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "    totalLosses = all[:,0].sum()\n",
    "    giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    "\n",
    "    giniSum -= (len(actual) + 1) / 2.\n",
    "    return giniSum / len(actual)\n",
    "\n",
    "def gini_normalized(a, p):\n",
    "    return gini(a, p) / gini(a, a)\n",
    "\n",
    "    \n",
    "\n",
    "predicted_y = np.array([0.98,0.85,0.8,0.9,0.2,0.115,0.9,0.9])\n",
    "desired_y = np.array([1,1,0.1,0.1,0.1,0.1,0.9,0.9])\n",
    "\n",
    "\n",
    "print (gini_normalized( desired_y,predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Text</th>\n",
       "      <th>Variation</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>Cyclin-dependent kinases (CDKs) regulate a var...</td>\n",
       "      <td>Truncating Mutations</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>CBL</td>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "      <td>W802*</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CBL</td>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "      <td>Q249E</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>CBL</td>\n",
       "      <td>Recent evidence has demonstrated that acquired...</td>\n",
       "      <td>N454D</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>CBL</td>\n",
       "      <td>Oncogenic mutations in the monomeric Casitas B...</td>\n",
       "      <td>L399V</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Class    Gene                                               Text  \\\n",
       "0     1  FAM58A  Cyclin-dependent kinases (CDKs) regulate a var...   \n",
       "1     2     CBL   Abstract Background  Non-small cell lung canc...   \n",
       "2     2     CBL   Abstract Background  Non-small cell lung canc...   \n",
       "3     3     CBL  Recent evidence has demonstrated that acquired...   \n",
       "4     4     CBL  Oncogenic mutations in the monomeric Casitas B...   \n",
       "\n",
       "              Variation  ID  \n",
       "0  Truncating Mutations   0  \n",
       "1                 W802*   1  \n",
       "2                 Q249E   2  \n",
       "3                 N454D   3  \n",
       "4                 L399V   4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = pd.merge(test1, testx1, how='left', on='ID').fillna('')\n",
    "train = pd.merge(train, trainx, how='left', on='ID').fillna('')\n",
    "sol[\"Class\"]=0\n",
    "for i in range(9):\n",
    "    sol[\"Class\"][sol[sol[\"class\"+str(i+1)]==1].index]=i+1\n",
    "sol=sol.drop([\"class\"+str(i+1) for i in range(9)],axis=1)\n",
    "\n",
    "\n",
    "test1=pd.merge(test1,sol, how='left', on='ID').fillna(\"iss\")\n",
    "test1=test1.drop(test1[test1[\"Class\"]==\"iss\"].index)\n",
    "test1=test1.drop(\"ID\",axis=1)\n",
    "train=train.drop(\"ID\",axis=1)\n",
    "train=pd.concat([train,test1])\n",
    "train[\"ID\"]=[i for i in range(train.shape[0])]\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3689, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "y = train['Class'].values\n",
    "train = train.drop(['Class'], axis=1)\n",
    "\n",
    "test = pd.merge(test, testx, how='left', on='ID').fillna('')\n",
    "pid = test['ID'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene</th>\n",
       "      <th>Text</th>\n",
       "      <th>Variation</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FAM58A</td>\n",
       "      <td>Cyclin-dependent kinases (CDKs) regulate a var...</td>\n",
       "      <td>Truncating Mutations</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CBL</td>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "      <td>W802*</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CBL</td>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "      <td>Q249E</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CBL</td>\n",
       "      <td>Recent evidence has demonstrated that acquired...</td>\n",
       "      <td>N454D</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CBL</td>\n",
       "      <td>Oncogenic mutations in the monomeric Casitas B...</td>\n",
       "      <td>L399V</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Gene                                               Text  \\\n",
       "0  FAM58A  Cyclin-dependent kinases (CDKs) regulate a var...   \n",
       "1     CBL   Abstract Background  Non-small cell lung canc...   \n",
       "2     CBL   Abstract Background  Non-small cell lung canc...   \n",
       "3     CBL  Recent evidence has demonstrated that acquired...   \n",
       "4     CBL  Oncogenic mutations in the monomeric Casitas B...   \n",
       "\n",
       "              Variation  ID  \n",
       "0  Truncating Mutations   0  \n",
       "1                 W802*   1  \n",
       "2                 Q249E   2  \n",
       "3                 N454D   3  \n",
       "4                 L399V   4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a57c12f5-73c2-415a-9cd0-3adaed0ddd2c",
    "_uuid": "22e5d25aa11c98d6f913d975cd51d436f8b64351",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fd7f7d96-f093-4321-8b60-e734c9d1794c",
    "_uuid": "145564b8cf0753323450a93fd5202c458c49d1b6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e24024d6-c577-4099-8c4b-ed728bb3da82",
    "_uuid": "42a876f902870ca8f1de3d0863dbc33921fac24b",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "391d7e9a-96dd-454b-924f-0876bf6cefee",
    "_uuid": "17c2a7ed868ae06ac1e15f8c90e19e4b17d321ad",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1edd1f68-0353-4f92-a739-db42d2adf265",
    "_uuid": "4a26635cd88839c392aa1019e60b42b369ab3bd2"
   },
   "source": [
    "### 1. Not use the codes below.\n",
    "***\n",
    "Not used in this Kernel.\n",
    "\n",
    "```python\n",
    "# commented for Kaggle Limits\n",
    "for i in range(56):\n",
    "    df_all['Gene_'+str(i)] = df_all['Gene'].map(lambda x: str(x[i]) if len(x)>i else '')\n",
    "    df_all['Variation'+str(i)] = df_all['Variation'].map(lambda x: str(x[i]) if len(x)>i else '')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "90833673-de06-422c-8df2-13ffe5911a5c",
    "_uuid": "86a1fbd09654d814e68622b05a327843290c7304",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "df_all['Gene_Share'] = df_all.apply(lambda r: sum([1 for w in r['Gene'].split(' ') if w in r['Text'].split(' ')]), axis=1)\n",
    "df_all['Variation_Share'] = df_all.apply(lambda r: sum([1 for w in r['Variation'].split(' ') if w in r['Text'].split(' ')]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9527f162-c93d-433d-a834-9c0215c45069",
    "_uuid": "4182af290004b8736d345f0e1a64f3e8969daf5d",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "b923b200-2d21-4166-afc9-0f39f167d86e",
    "_uuid": "158f5e7bfe484e1f006aefdfe681e4f2ac8db6cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3578\n"
     ]
    }
   ],
   "source": [
    "gen_var_lst = sorted(list(train.Gene.unique()) + list(train.Variation.unique()))\n",
    "print(len(gen_var_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "0dc59167-7e16-43ac-bdc4-d0a48dcbd296",
    "_uuid": "1a42b0fc3a8b361064c8f73fdfe5ed6525f85eba",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3397\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n"
     ]
    }
   ],
   "source": [
    "gen_var_lst = [x for x in gen_var_lst if len(x.split(' '))==1]\n",
    "print(len(gen_var_lst))\n",
    "i_ = 0\n",
    "\n",
    "#commented for Kaggle Limits\n",
    "for gen_var_lst_itm in gen_var_lst:\n",
    "    if i_ % 100 == 0: print(i_)\n",
    "    df_all['GV_'+str(gen_var_lst_itm)] = df_all['Text'].map(lambda x: str(x).count(str(gen_var_lst_itm)))\n",
    "    i_ += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "38cca869-3d4d-44f6-8c55-b78bb2840609",
    "_uuid": "199df6688fc7654251228cd85bb5dcd87fd5ebda",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in df_all.columns:\n",
    "    if df_all[c].dtype == 'object':\n",
    "        if c in ['Gene','Variation']:\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            df_all[c+'_lbl_enc'] = lbl.fit_transform(df_all[c].values)  \n",
    "            df_all[c+'_len'] = df_all[c].map(lambda x: len(str(x)))\n",
    "            df_all[c+'_words'] = df_all[c].map(lambda x: len(str(x).split(' ')))\n",
    "        elif c != 'Text':\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            df_all[c] = lbl.fit_transform(df_all[c].values)\n",
    "        if c=='Text': \n",
    "            df_all[c+'_len'] = df_all[c].map(lambda x: len(str(x)))\n",
    "            df_all[c+'_words'] = df_all[c].map(lambda x: len(str(x).split(' '))) \n",
    "\n",
    "train = df_all.iloc[:len(train)]\n",
    "test = df_all.iloc[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "ac511901-464b-4d08-aeba-1be18edb9661",
    "_uuid": "570ce7b3ab8b9f46c51843ea9d5c103c24dbb06e",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-058d1d044437>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3b870664-6bda-49f9-8f1d-9fc85072ef3d",
    "_uuid": "3050f029fe7406337d4946b834c05124eaff2b76",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "03a1bc89-a8dd-4e0b-9fb2-e4ddc8286fa6",
    "_uuid": "a6406d11c0298511b4107abbbe0fa947de522791",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bba97217-b7ee-4747-b1bc-71e68d2d57fa",
    "_uuid": "2d13c3b056df19eef7fac1416a9b27c01f2723fe",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "ae2a7760-2706-4342-a0f5-03212584b9ae",
    "_uuid": "7e2a2ec9fd7908f36000470699c19b4425fc6181",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class cust_regression_vals(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, x):\n",
    "        x = x.drop(['Gene', 'Variation','ID','Text'],axis=1).values\n",
    "        return x\n",
    "\n",
    "class cust_txt_col(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, x):\n",
    "        return x[self.key].apply(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "36663a09-da14-4df0-87ec-9fe8771abf6c",
    "_uuid": "9b62f6d90b60d79c974b2f61a5d374e3412de7f2"
   },
   "source": [
    "### 2. Main difference\n",
    "***\n",
    "#### 1. Pipeline Changed\n",
    "\n",
    "The original Kernel uses the pipeline with these codes below for 'Text' feature extraction:\n",
    "```python\n",
    "#commented for Kaggle Limits\n",
    "('pi3', pipeline.Pipeline([('Text', cust_txt_col('Text')), \n",
    "                           ('tfidf_Text', feature_extraction.text.TfidfVectorizer(ngram_range=(1, 2))), \n",
    "                           ('tsvd3', decomposition.TruncatedSVD(n_components=50, n_iter=25, random_state=12))]))\n",
    "```\n",
    "Unfortunately, it can not fit my memory of 8GB + 2GB(swap). And without these features, I can only get nearly 0.7xxx on PL.\n",
    "\n",
    "So, I try to use **HashingVectorizer + TfidfTransformer** instead of **TfidfVectorizer**. [Reference](http://scikit-learn.org/stable/modules/feature_extraction.html#vectorizing-a-large-text-corpus-with-the-hashing-trick)\n",
    "\n",
    "#### 2. Parameter Tuning\n",
    "\n",
    "For HashingVectorizer saved my memory, I try to use **ngram_range=(1, 3)** with HashingVectorizer. \n",
    "\n",
    "And **n_components=300** with **TruncatedSVD**.\n",
    "\n",
    "#### 3. Batch Transform\n",
    "\n",
    "With these codes, I can fit_transform the **train**, but still out of memory if transform **test**.\n",
    "\n",
    "So, I try to use batch transform of test data step by step, and vstack all. \n",
    "\n",
    "And, it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "6f895147-c745-4792-9a43-f6bd6ff7822c",
    "_uuid": "5fcca871cc477a821fa964d87f46a314b168dfa5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline...\n"
     ]
    }
   ],
   "source": [
    "import function as fun\n",
    "print('Pipeline...')\n",
    "fp = pipeline.Pipeline([\n",
    "    ('union', pipeline.FeatureUnion(\n",
    "        n_jobs = 3,\n",
    "        transformer_list = [\n",
    "            ('standard', fun.cust_regression_vals()),\n",
    "            ('pi1', pipeline.Pipeline([('Gene', fun.cust_txt_col('Gene')), \n",
    "                                       ('count_Gene', feature_extraction.text.CountVectorizer(analyzer=u'char', ngram_range=(1, 8))), \n",
    "                                       ('tsvd1', decomposition.TruncatedSVD(n_components=20, n_iter=25, random_state=12))])),\n",
    "            ('pi2', pipeline.Pipeline([('Variation', fun.cust_txt_col('Variation')), \n",
    "                                       ('count_Variation', feature_extraction.text.CountVectorizer(analyzer=u'char', ngram_range=(1, 8))), \n",
    "                                       ('tsvd2', decomposition.TruncatedSVD(n_components=20, n_iter=25, random_state=12))])),\n",
    "            #commented for Kaggle Limits\n",
    "             ('pi3', pipeline.Pipeline([('Text', fun.cust_txt_col('Text')), \n",
    "                                        ('hv', feature_extraction.text.HashingVectorizer(decode_error='ignore', n_features=2 ** 16, non_negative=True, ngram_range=(1, 3))),\n",
    "                                        ('tfidf_Text', feature_extraction.text.TfidfTransformer()), \n",
    "                                        ('tsvd3', decomposition.TruncatedSVD(n_components=300, n_iter=25, random_state=12))]))\n",
    "\n",
    "        \n",
    "        ])\n",
    "    )])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "(3689, 3747)\n",
      "(986, 3747)\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")\n",
    "train = fp.fit_transform(train)\n",
    "print (train.shape)\n",
    "\n",
    "test_t = np.empty([0, train.shape[1]])\n",
    "step = 200\n",
    "for i in range(0, len(test), step):\n",
    "    step_end = i+step\n",
    "    step_end = step_end if step_end < len(test) else len(test)\n",
    "    _test = fp.transform(test.iloc[i:step_end])\n",
    "    test_t = np.vstack((test_t, _test))\n",
    "test = test_t\n",
    "print (test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d373d666-f0a9-4f84-aa34-26c5674f4736",
    "_uuid": "c85df64546e595beb036aa357b6135a063fc3d0b"
   },
   "source": [
    "### 3. Xgboost Parameter Tuning\n",
    "***\n",
    "#### 1. eta 0.03333 -> 0.02 \n",
    "\n",
    "I like small learning rate.\n",
    "\n",
    "#### 2. max_depth 4 -> 6 \n",
    "\n",
    "Bigger means higher risk of overfitting. But, since we got more features, maybe it could be better for this big data?! I'm not sure yet.\n",
    "\n",
    "#### 3. test_size 0.18 -> 0.15\n",
    "\n",
    "With slightly more data to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "7af95c64-27c7-4c86-9a72-775af3c3a9a2",
    "_uuid": "2d1c0b506fb71abf8b6ac60f5f51d91504db209f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = y - 1 #fix for zero bound array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "2ba02c19-f3f5-4502-b976-3768453a2c1a",
    "_uuid": "4e5ae29c6ac788f0d4d157d8ffde170e58023a82",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.15584\tvalid-mlogloss:2.16359\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 100 rounds.\n",
      "[50]\ttrain-mlogloss:1.15495\tvalid-mlogloss:1.38552\n",
      "[100]\ttrain-mlogloss:0.776327\tvalid-mlogloss:1.12099\n",
      "[150]\ttrain-mlogloss:0.568953\tvalid-mlogloss:1.00452\n",
      "[200]\ttrain-mlogloss:0.44558\tvalid-mlogloss:0.950147\n",
      "[250]\ttrain-mlogloss:0.354375\tvalid-mlogloss:0.923852\n",
      "[300]\ttrain-mlogloss:0.283331\tvalid-mlogloss:0.911602\n",
      "[350]\ttrain-mlogloss:0.229348\tvalid-mlogloss:0.907653\n",
      "[400]\ttrain-mlogloss:0.185288\tvalid-mlogloss:0.909563\n",
      "[450]\ttrain-mlogloss:0.151876\tvalid-mlogloss:0.917215\n",
      "Stopping. Best iteration:\n",
      "[355]\ttrain-mlogloss:0.224587\tvalid-mlogloss:0.907363\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The type of target data is not known",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-248b490a20ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mwatchlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'valid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mwatchlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mscore1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mntree_limit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_ntree_limit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m#if score < 0.9:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mlog_loss\u001b[1;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[0;32m   1626\u001b[0m                              'got {0}.'.format(lb.classes_))\n\u001b[0;32m   1627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1628\u001b[1;33m     \u001b[0mtransformed_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1630\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtransformed_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    333\u001b[0m                               \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m                               \u001b[0mneg_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneg_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m                               sparse_output=self.sparse_output)\n\u001b[0m\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py\u001b[0m in \u001b[0;36mlabel_binarize\u001b[1;34m(y, classes, neg_label, pos_label, sparse_output)\u001b[0m\n\u001b[0;32m    472\u001b[0m                          \"binarization\")\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'unknown'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The type of target data is not known\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The type of target data is not known"
     ]
    }
   ],
   "source": [
    "\n",
    "file_pre = datetime.datetime.now().strftime('%m_%d_%H_%M_%S')\n",
    "\n",
    "denom = 0\n",
    "fold = 30 #Change to , 1 for Kaggle Limits\n",
    "for i in range(fold):\n",
    "    params = {\n",
    "        #'eta': 0.03333,\n",
    "        'eta': 0.02,\n",
    "        #'max_depth': 4,\n",
    "        'max_depth': 6,\n",
    "        'objective': 'multi:softprob',\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'num_class': 9,\n",
    "        'seed': i,\n",
    "        'silent': True\n",
    "    }\n",
    "    x1, x2, y1, y2 = model_selection.train_test_split(train, y, test_size=0.15, random_state=i)\n",
    "    watchlist = [(xgb.DMatrix(x1, y1), 'train'), (xgb.DMatrix(x2, y2), 'valid')]\n",
    "    model = xgb.train(params, xgb.DMatrix(x1, y1), 1000,  watchlist, verbose_eval=50, early_stopping_rounds=100)\n",
    "    score1 = metrics.log_loss(y2, model.predict(xgb.DMatrix(x2), ntree_limit=model.best_ntree_limit), labels = list(range(9)))\n",
    "    print(score1)\n",
    "    #if score < 0.9:\n",
    "    if denom != 0:\n",
    "        pred = model.predict(xgb.DMatrix(test), ntree_limit=model.best_ntree_limit+80)\n",
    "        preds += pred\n",
    "    else:\n",
    "        pred = model.predict(xgb.DMatrix(test), ntree_limit=model.best_ntree_limit+80)\n",
    "        preds = pred.copy()\n",
    "    denom += 1\n",
    "    submission = pd.DataFrame(pred, columns=['class'+str(c+1) for c in range(9)])\n",
    "    submission['ID'] = pid\n",
    "    submission.to_csv('submission_xgb_fold_'  + str(i) + '_' + file_pre + '.csv', index=False)\n",
    "preds /= denom\n",
    "submission = pd.DataFrame(preds, columns=['class'+str(c+1) for c in range(9)])\n",
    "submission['ID'] = pid\n",
    "submission.to_csv('submission_xgb_' + file_pre + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xgboost.core.DMatrix at 0x214e1ba47f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.DMatrix(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e68d420c-618b-4936-9528-e21d9d993472",
    "_uuid": "6c53a32d9150a8b3e531f978860eb59c54e3ea44",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (7.0, 7.0)\n",
    "xgb.plot_importance(booster=model,); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8c9d124c-2c43-433a-9876-9e9d4bd593b6",
    "_uuid": "44fb8218b1775155c09a92ec9fbd3dd9e4fd751d"
   },
   "source": [
    "### References\n",
    "* [Redefining Treatment](https://www.kaggle.com/the1owl/redefining-treatment-0-57456)\n",
    "* [Vectorizing a large text corpus with the hashing trick](http://scikit-learn.org/stable/modules/feature_extraction.html#vectorizing-a-large-text-corpus-with-the-hashing-trick)\n",
    "* [HashingVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "df9a7b7c-0f33-41c1-b188-92ec405ef626",
    "_uuid": "0f62da96ffd443abab31de7de5c14fdb2844236b",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
